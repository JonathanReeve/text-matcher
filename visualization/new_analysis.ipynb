{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Text Matching Data Generated from JSTOR Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import spacy\n",
    "import re\n",
    "import json\n",
    "import altair as alt\n",
    "#new viz library for single-column heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "#from nltk.corpus import names\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 6]\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nocite_pages.txt') as f: \n",
    "    mm = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textALength = len(mm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chapter locations\n",
    "chapterMatches = re.finditer('~', mm)\n",
    "chapterLocations = [match.start() for match in chapterMatches]\n",
    "chapterLocations.append(textALength) # Add one to account for last chunk. \n",
    "len(chapterLocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getChapters(text): \n",
    "    chapters = []\n",
    "    for i, loc in enumerate(chapterLocations): \n",
    "        if i != len(chapterLocations)-1: \n",
    "            chapter = mm[loc:chapterLocations[i+1]]\n",
    "            chapters.append(chapter)\n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapters = getChapters(mm)\n",
    "chapterLengths = [len(chapter.split()) for chapter in chapters]\n",
    "chapterLengthsSeries = pd.Series(chapterLengths)\n",
    "chapterLengthsSeries.plot(kind='bar', title='Chapter Lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('nocite.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Decade'] = df['publicationYear'] - (df['publicationYear'] % 10)\n",
    "# df['Locations in A'] = df['matches'].apply(lambda x: x[1])\n",
    "# df['NumMatches'] = df['matches'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([len(item) for item in df['Locations in A'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many articles do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df) # Total articles with \"Middlemarch\" mentioned somewhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find only those with non-trivial quotations from Middlemarch: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches = df[df['Locations in A'].apply(lambda x: len(x) > 0)]\n",
    "#articlesWithMatches.year.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches.Wordcounts.apply(len).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articlesWithMatches.to_json('../data/cleaned-matches.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many articles do we have published in each year? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(articlesWithMatches).mark_bar().encode(x='publicationYear:O', y='count()').properties(width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Numbers of Quoted Words Per Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quoted Words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Quoted Words'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['Quoted Words'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Quoted Words'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats about Wordcounts\n",
    "\n",
    "Average number of words per match, per item: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Wordcounts'].apply(np.mean).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Wordcounts'].apply(np.mean).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for extracting wordcounts, numbers of quotations for diachronic and synchronic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diachronicAnalysis(df, decades=(1950, 2020), bins=chapterLocations, useWordcounts=True, normalize=True):\n",
    "    \"\"\" Turning on useWordcounts makes it so that it's weighted by wordcount. \n",
    "    Turning it off uses raw numbers of quotations. \"\"\"\n",
    "    decades = np.arange(decades[0], decades[1], 10)\n",
    "    # Make a dictionary of decades. \n",
    "    # Values are a list of locations.  \n",
    "    decadeDict = {}\n",
    "    for i, row in df.iterrows():\n",
    "        decade = row['Decade']\n",
    "        locationsAndWordcounts = row['Locations in A with Wordcounts']\n",
    "        if decade not in decadeDict: \n",
    "            decadeDict[decade] = locationsAndWordcounts.copy()\n",
    "        else: \n",
    "            decadeDict[decade] += locationsAndWordcounts.copy()\n",
    "    # Grab the beginnings of quotes. \n",
    "    decadeStartsWeights = {decade: [(item[0][0], item[1]) \n",
    "                                    for item in loc] \n",
    "                    for decade, loc in decadeDict.items()}\n",
    "    if useWordcounts: \n",
    "        decadesBinned = {decade: \n",
    "                     np.histogram([loc[0] for loc in locations], \n",
    "                                  bins=bins,\n",
    "                                  weights=[loc[1] for loc in locations],\n",
    "                                  range=(0, textALength))[0]\n",
    "                     for decade, locations in decadeStartsWeights.items() \n",
    "                         if decade in decades}\n",
    "    else: \n",
    "        decadesBinned = {decade: \n",
    "                     np.histogram([loc[0] for loc in locations], \n",
    "                                  bins=bins,\n",
    "                                  range=(0, textALength))[0]\n",
    "                     for decade, locations in decadeStartsWeights.items() \n",
    "                         if decade in decades}\n",
    "    decadesDF = pd.DataFrame(decadesBinned).T\n",
    "    #Normalize\n",
    "    if normalize: \n",
    "        decadesDF = decadesDF.div(decadesDF.max(axis=1), axis=0)\n",
    "    return decadesDF\n",
    "\n",
    "def countWords(locRange): \n",
    "    \"\"\" Counts words in middlemarch, given character ranges. \"\"\"\n",
    "    \n",
    "    chunk = mm[locRange[0]:locRange[1]]\n",
    "    return len(chunk.split())\n",
    "\n",
    "def totalWords(locRangeSet): \n",
    "    \"\"\" Counts total words in a list of location ranges. \"\"\"\n",
    "    locRangeSet = locRangeSet\n",
    "    return sum([countWords(locRange) for locRange in locRangeSet])    \n",
    "    \n",
    "def countsPerSet(locRangeSet): \n",
    "    \"\"\" Returns an augmented location range set that includes word counts. \"\"\"\n",
    "    locRangeSet = locRangeSet\n",
    "    return [(locRange, countWords(locRange))\n",
    "             for locRange in locRangeSet]\n",
    "    \n",
    "def extractWordcounts(locsAndWordcounts): \n",
    "    \"\"\" \n",
    "    Takes pairs of location ranges and wordcounts, \n",
    "    and returns just the wordcounts. \n",
    "    \"\"\"\n",
    "    return [item[1] for item in locsAndWordcounts \n",
    "            if len(locsAndWordcounts) > 0]\n",
    "\n",
    "def synchronicAnalysis(df, bins=chapterLocations, useWordcounts=True): \n",
    "    locs = df['Locations in A'].values\n",
    "    locCounts = [(loc, countWords(loc)) for locSet in locs\n",
    "              for loc in locSet]\n",
    "    starts = [loc[0][0] for loc in locCounts]\n",
    "    counts = [loc[1] for loc in locCounts]\n",
    "    if useWordcounts: \n",
    "        binned = np.histogram(starts, bins=bins, \n",
    "                              weights=counts, range=(0, textALength))\n",
    "    else: \n",
    "        binned = np.histogram(starts, bins=bins, \n",
    "                              range=(0, textALength))\n",
    "    binnedDF = pd.Series(binned[0])\n",
    "    return binnedDF\n",
    "\n",
    "def plotDiachronicAnalysis(df, save=False, reverse=False): \n",
    "    ylabels = [str(int(decade)) for decade in df.index] + ['2020']\n",
    "    plt.pcolor(df, cmap='gnuplot')\n",
    "    plt.yticks(np.arange(len(df.index)+1), ylabels)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.ylabel('Decade')\n",
    "    plt.xlabel('Chapter')\n",
    "    plt.gca().set_xlim((0, len(df.T)))\n",
    "    plt.colorbar(ticks=[])\n",
    "    if save: \n",
    "        plt.savefig('diachronic.png', bboxinches='tight', dpi=300, transparent=True)\n",
    "    plt.show()\n",
    "    \n",
    "def plotSynchronicAnalysis(s, useWordcounts=True): \n",
    "    ax = s.plot(kind='bar')\n",
    "    ax.set_xlabel('Chapter')\n",
    "    if useWordcounts: \n",
    "        ax.set_ylabel('Number of Words Quoted')\n",
    "    else: \n",
    "        ax.set_ylabel('Number of Quotations')\n",
    "    plt.locator_params('x', nbins = 20)\n",
    "        \n",
    "def plotSynchronicAnalysisHeatmap(s, useWordcounts=True): \n",
    "    vec1=synchronicAnalysis(df, useWordcounts=False)\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.color_palette(\"magma\")\n",
    "    sns.heatmap([vec1])\n",
    "    ax.set_xlabel('Chapter')\n",
    "    ax.set_ylabel('Number of Quotations')\n",
    "    plt.locator_params('x', nbins = 20)\n",
    "    \n",
    "def plotDiachronicAnalysisBubble(df, save=False, reverse=False):\n",
    "    ylabels = [str(int(decade)) for decade in df.index] + ['2020'] \n",
    "    alt.Chart(df).mark_circle().encode(\n",
    "    x='Chapter',\n",
    "    y='Decade',\n",
    "    size='sum(count):Q'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Quoted Words'] = df['Locations in A'].apply(totalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Locations in A with Wordcounts'] = df['Locations in A'].apply(countsPerSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the diachronic wordcounts are the same as the synchronic wordcounts\n",
    "decadeSums = diachronicAnalysis(df, decades=(1700, 2020), useWordcounts=True, normalize=False).sum(axis=1)\n",
    "decadeSums.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chapterSums = synchronicAnalysis(df)\n",
    "chapterSums.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quotation Length Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wordcounts'] = df['Locations in A with Wordcounts'].apply(extractWordcounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = []\n",
    "for countSet in df['Wordcounts'].values: \n",
    "    for count in countSet: \n",
    "        wordcounts.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(wordcounts).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Quotes (and words Quoted) by Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSynchronicAnalysis(synchronicAnalysis(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synchronicAnalysis(df, useWordcounts=True).to_csv('test_pages.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allMatches = []\n",
    "for group in df['Locations in A'].values: \n",
    "    for pair in group: \n",
    "        allMatches.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(allMatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSynchronicAnalysis(synchronicAnalysis(df, useWordcounts=False), useWordcounts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotationsPerChapter = synchronicAnalysis(df, bins=chapterLocations, useWordcounts=False)\n",
    "quotationsPerChapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotationsPerChapter = pd.DataFrame(quotationsPerChapter, index=range(0,249), columns=['Number of Quotations'])\n",
    "quotationsPerChapter['Chapter'] = range(0, 249)\n",
    "quotationsPerChapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(quotationsPerChapter).mark_circle().encode(x='Chapter:O', size='Number of Quotations:Q').properties(width=1000, height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(quotationsPerChapter).mark_circle().encode(x='Chapter:O', size=alt.Size('Number of Quotations:Q', scale=alt.Scale(range=[1, 1000]))).properties(width=1000, height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo chart with horizontal labels\n",
    "alt.Chart(quotationsPerChapter).mark_circle().encode(x=alt.X('Chapter:Q', axis=alt.Axis(title=\"Chapter\", tickMinStep=5,\n",
    "                                       labelOverlap=False,labelAngle=0)), \n",
    "size=alt.Size('Number of Quotations:Q', scale=alt.Scale(range=[1, 1000]))).properties(width=1000,height=150).configure_legend(\n",
    "    titleFontSize=9,\n",
    "    labelFontSize=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(quotationsPerChapter).mark_circle().encode(y='Chapter:O', size=alt.Size('Number of Quotations:Q', scale=alt.Scale(range=[1, 1000]))).properties(width=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Number of Quotations Per Chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw number of quotations per chapter\n",
    "# synchronicAnalysis(df, useWordcounts=False).to_csv('../papers/spring2017-middlemarch-paper/data/num-quotations-per-chapter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted for the number of words in each chapter\n",
    "ax = (synchronicAnalysis(df) / chapterLengthsSeries).plot(kind='bar')\n",
    "ax.set_xlabel('Chapter')\n",
    "ax.set_ylabel('Words Quoted, Normalized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDiachronicAnalysis(diachronicAnalysis(df, decades=(1950, 2020), bins=chapterLocations).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDiachronicAnalysis(diachronicAnalysis(df, decades=(1960, 2020), bins=chapterLocations).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional heatmap of the synchonic raw number of quotations per chapter, as heatmap\n",
    "vec1=synchronicAnalysis(df, useWordcounts=False)\n",
    "fig, ax = plt.subplots()\n",
    "sns.color_palette(\"magma\")\n",
    "sns.heatmap([vec1])\n",
    "ax.set_xlabel('Chapter')\n",
    "ax.set_ylabel('Number of Quotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a one-dimensional heatmap of the synchonic raw number of quotations per chapter, as heatmap\n",
    "# INVERTED COLOR SCHEMA\n",
    "vec1=synchronicAnalysis(df, useWordcounts=False)\n",
    "fig, ax = plt.subplots()\n",
    "sns.heatmap([vec1], cmap = 'magma_r')\n",
    "ax.set_xlabel('Chapter')\n",
    "ax.set_ylabel('Number of Quotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDF = diachronicAnalysis(df, decades=(1960, 2020), bins=chapterLocations).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDFquoteOnly = diachronicAnalysis(df, decades=(1960, 2020), bins=chapterLocations, useWordcounts=False, normalize=False).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotDiachronicAnalysisBubble(diachronicAnalysis(df, decades=(1960, 2020), bins=chapterLocations).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synDF = synchronicAnalysis(df, useWordcounts=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synDF.index.name = 'chapter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redo chart in Altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDF.columns.name = 'chapter'\n",
    "diaDF.index.name = 'decade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo with raw quotations, not normalized by decade\n",
    "diaDFquoteOnly.columns.name ='chapter'\n",
    "diaDFquoteOnly.index.name = 'decade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDFquoteOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDF['decade'] = diaDF.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaMelted = diaDF.melt(id_vars='decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDFquoteOnly['decade'] = diaDFquoteOnly.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diaDFquoteOnlyMelted = diaDFquoteOnly.melt(id_vars='decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(diaMelted).mark_rect().encode(x='chapter:O', y='decade:O', color=alt.Color('value', legend=alt.Legend(title=\"# of Quotations (normalized)\"))).properties(width=1000, height=300).configure(background='#eeeeeeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(diaMelted).mark_circle().encode(x='chapter:O', y='decade:O', size=alt.Size('value',  legend=alt.Legend(title=\"Number of Quotations (normalized)\"), scale=alt.Scale(type = 'threshold', domain = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], range =[0, 20, 60, 100, 150, 250, 350, 500, 750, 1000, 1500, 2000,]))).properties(width=1000, height=300).configure_legend(\n",
    "titleFontSize=9,\n",
    "labelFontSize=10\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Redo Chart to rotate tick marks\n",
    "alt.Chart(diaMelted).mark_circle().encode(\n",
    "    x=alt.X('chapter:Q', axis=alt.Axis(tickMinStep=5,\n",
    "                                       labelOverlap=False,\n",
    "                                   labelAngle=0)), \n",
    "    y=alt.Y('decade:O'), \n",
    "    size=alt.Size('value',  legend=alt.Legend(title=\"Number of Quotations (normalized)\"), \n",
    "                  scale=alt.Scale(type = 'threshold', domain = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1], range =[0, 20, 60, 100, 150, 250, 350, 500, 750, 1000, 1500, 2000,]))).properties(width=1000, height=300).configure_legend(\n",
    "titleFontSize=9,\n",
    "labelFontSize=10\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chart with raw quotations\n",
    "alt.Chart(diaDFquoteOnlyMelted).mark_rect().encode(x='chapter:O', y='decade:O', color='value').properties(width=1000, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(diaDFquoteOnlyMelted).mark_circle().encode(x='chapter:O', y='decade:O', size='value').properties(width=1000, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized proportion of, say, Chapter 20 in 1950: \n",
    "diachronicAnalysis(df)[20][1950]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By (Guessed) Gender of Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maleNames, femaleNames = names.words('male.txt'), names.words('female.txt')\n",
    "maleNames = [name.lower() for name in maleNames]\n",
    "femaleNames = [name.lower() for name in femaleNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guessGender(name): \n",
    "    name = name.split()[0].lower() # Grab the first name. \n",
    "    if name in maleNames and name in femaleNames: \n",
    "        return 'A' #Ambiguous\n",
    "    elif name in maleNames: \n",
    "        return 'M'\n",
    "    elif name in femaleNames: \n",
    "        return 'F'\n",
    "    else: \n",
    "        return 'U'\n",
    "\n",
    "def averageGender(names): \n",
    "    if type(names) != list: \n",
    "        return 'U'\n",
    "    genderGuesses = [guessGender(name) for name in names]\n",
    "    stats = Counter(genderGuesses).most_common()\n",
    "    if len(stats) == 1: \n",
    "        # Only one author. We can just use that's author's gender guess. \n",
    "        return stats[0][0]\n",
    "    elif stats[0][1] == stats[1][1]: # There's a tie. \n",
    "        return 'A' # Ambiguous. \n",
    "    else: \n",
    "        return stats[0][0] # Return the most common gender. \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['gender'] = df['author'].apply(averageGender)\n",
    "dfF = df.loc[df['gender'] == 'F']\n",
    "dfM = df.loc[df['gender'] == 'M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differences in citations between genders. \n",
    "plotSynchronicAnalysis(synchronicAnalysis(dfM) - synchronicAnalysis(dfF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By (Guessed) Country of Publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFirst(row): \n",
    "    if type(row) == list: \n",
    "        return row[0]\n",
    "    else: \n",
    "        return row\n",
    "\n",
    "topPublishers = df['publisher_name'].apply(getFirst).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = topPublishers[:80].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = publishers.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountry(publisher): \n",
    "    brits = ['Oxford University Press', 'Cambridge University Press', 'Modern Humanities Research Association', \\\n",
    "             'BMJ', 'Taylor & Francis, Ltd.', 'Edinburgh University Press', \\\n",
    "             'Royal Society for the Encouragement of Arts, Manufactures and Commerce']\n",
    "    canadians = ['Victorian Studies Association of Western Canada'] \n",
    "    if type(publisher) != list: \n",
    "        return 'Unknown'\n",
    "    publisher = publisher[0]\n",
    "    if publisher in brits: \n",
    "        return 'Britain' \n",
    "    elif publisher in canadians or 'Canada' in publisher: \n",
    "        return 'Canada' \n",
    "    elif 'GmbH' in publisher: \n",
    "        return 'Germany'\n",
    "    elif 'estudios' in publisher: \n",
    "        return 'Spain'\n",
    "    elif 'France' in publisher: \n",
    "        return 'France' \n",
    "    elif 'Ireland' in publisher: \n",
    "        return 'Ireland'\n",
    "    else: \n",
    "        return 'US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['country'] = df['publisher_name'].apply(getCountry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBrits = df.loc[df['country'] == 'Britain']\n",
    "dfYanks = df.loc[df['country'] == 'US']\n",
    "dfCanadians = df.loc[df['country'] == 'Canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since British authors are greatly outnumbered in this corpus, we should normalize the data. \n",
    "britsHist = synchronicAnalysis(dfBrits) \n",
    "normBrits = britsHist.div(britsHist.max())\n",
    "yanksHist = synchronicAnalysis(dfYanks)\n",
    "normYanks = yanksHist.div(yanksHist.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSynchronicAnalysis(normYanks - normBrits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top journals. \n",
    "journalStats = df['isPartOf'].value_counts()\n",
    "journalStats[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journalList = journalStats.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the specialist journal, \"George Eliot - George Henry Lewes Studies,\" with all other journals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geJournals = df.loc[df['isPartOf'] == 'George Eliot - George Henry Lewes Studies']\n",
    "otherJournals = df.loc[df['isPartOf'] != 'George Eliot - George Henry Lewes Studies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "geDF = synchronicAnalysis(geJournals)\n",
    "otherDF = synchronicAnalysis(otherJournals)\n",
    "normGE = geDF.div(geDF.max())\n",
    "normOther = otherDF.div(otherDF.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = (normGE - normOther).plot(kind='bar')\n",
    "fig.add_subplot(ax)\n",
    "ax.set_xlabel('Chapter')\n",
    "ax.set_ylabel('Specialization Index')\n",
    "# Save a big version for publication. \n",
    "#fig.savefig('specialization.png', bboxinches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals = pd.DataFrame({title: synchronicAnalysis(df.loc[df['isPartOf'] == title]) for title in journalList }).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 500\n",
    "topJournals = journals.loc[journals.sum(axis=1) > cutoff]\n",
    "otherJournals = journals.loc[journals.sum(axis=1) < cutoff]\n",
    "topJournals.loc['Other'] = otherJournals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topJournals.T.plot(kind='bar', stacked=True, colormap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = topJournals.T.plot(kind='bar', stacked=True, colormap='nipy_spectral')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('synchronic-journals.png', bboxinches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the total number of journals\n",
    "len(journalStats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour: Ch. 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find out why Ch. 15 was so big in the 80s and 90s. \n",
    "chap15s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1980, 1990]: \n",
    "        for start in starts: \n",
    "            if start > 290371 and start < 322052: # Does it cite Chapter XV? \n",
    "                if row.id not in ids: \n",
    "                    chap15s.append(row)\n",
    "                    ids.append(row.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the titles of those articles. \n",
    "[item.title for item in chap15s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ch15Topics =  [item.topics for item in chap15s]\n",
    "chap15TopicsFlat = [item for sublist in ch15Topics for item in sublist]\n",
    "Counter(chap15TopicsFlat).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvStart, xvEnd = chapterLocations[15:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm[xvStart:xvStart+1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find out which articles cite the first 2/3 of Chapter XV (with Lydgate's scientific research) \n",
    "# vs the last 1/3 on the story of Laure\n",
    "chap15p1s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1980, 1990]: \n",
    "        for start in starts: \n",
    "            if start > 290371 and start < 313892: # Does it cite the first 2/3 of Chapter XV? \n",
    "                if row.id not in ids: \n",
    "                    chap15p1s.append(row)\n",
    "                    ids.append(row.id)\n",
    "chap15p2s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1980, 1990]: \n",
    "        for start in starts: \n",
    "            if start > 313892 and start < 322052: # Does it cite the last 1/3 of Chapter XV? \n",
    "                if row.id not in ids: \n",
    "                    chap15p2s.append(row)\n",
    "                    ids.append(row.id)                   \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of articles citing the first 2/3 \n",
    "[item.title for item in chap15p1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of those articles. \n",
    "[item.title for item in chap15p2s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that we have the right location for the start of Laure's story in the last 1/3 of Chapter XV\n",
    "print(mm[313892:313892+1500]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the location of the eipgraph and first paragraph\n",
    "print(mm[290371:290371+1571]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap15para1s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1980, 1990]: \n",
    "        for start in starts: \n",
    "            if start > 290371 and start < 291943: # Does it cite the last 1/3 of Chapter XV? \n",
    "                if row.id not in ids: \n",
    "                    chap15para1s.append(row)\n",
    "                    ids.append(row.id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of articles that cite paragraph 1 of Chapter 15\n",
    "[item.title for item in chap15para1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chap15Lydgates = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1980, 1990]: \n",
    "        for start in starts: \n",
    "            if start > 291942 and start < 313892: # Does it cite the first 2/3 of Chapter XV? \n",
    "                if row.id not in ids: \n",
    "                    chap15Lydgates.append(row)\n",
    "                    ids.append(row.id)\n",
    "                    \n",
    "# Get the titles of articles that cite Lydgate section\n",
    "[item.title for item in chap15Lydgates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter 20 Detour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find out what articles cited chapter 20 \n",
    "chap20s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1870, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010]: \n",
    "        for start in starts: \n",
    "            if start > 406324 and start < 432778: # Does it cite Chapter XX? \n",
    "                if row.id not in ids: \n",
    "                    chap20s.append(row)\n",
    "                    ids.append(row.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of those articles. \n",
    "[item.title for item in chap20s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# articlesWithoutMatches.title #Print the titles of articles without matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chap20s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find out what articles cite paragraph 6 in Chapter 20\n",
    "chap20par6s = []\n",
    "ids = []\n",
    "for i, row in df.iterrows(): \n",
    "    locations = row['Locations in A']\n",
    "    starts = [item[0] for item in locations]\n",
    "    if row['Decade'] in [1870, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010]: \n",
    "        for start in starts: \n",
    "            if start > 411152 and start < 412177: # Does it cite Chapter XX? \n",
    "                if row.id not in ids: \n",
    "                    chap20par6s.append(row)\n",
    "                    ids.append(row.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the titles of those articles.\n",
    "[item.title for item in chap20par6s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chap20par6s) # The number of items citing paragraph 6 in chapter 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxStart, xxEnd = chapterLocations[20:22] # Chapter 20 Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm[xxStart:xxStart+1000]) # Verify we have Ch. 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = mm[xxStart:xxEnd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxParaLocations = [match.start() for match in re.finditer('\\n\\n+', mm)]\n",
    "xxParaLocations = [x for x in xxParaLocations if (x > xxStart) and (x < xxEnd)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm[xxParaLocations[4]:xxParaLocations[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Locations in A'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inXX(matches): \n",
    "    \"\"\" Determine if the article has a match in Ch. 20\"\"\"\n",
    "    for match in matches: \n",
    "        if match[0] > xxStart and match[0] < xxEnd:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['Locations in A'].apply(inXX).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraIndicesIn20(matches, paraLocations=xxParaLocations): \n",
    "    \"\"\" Determine paragraph number (index) for match in Ch. 20. \"\"\"\n",
    "    paraIndices = []\n",
    "    if inXX(matches): \n",
    "        paraBoundaries = list(zip(paraLocations, paraLocations[1:]))\n",
    "        for match in matches: \n",
    "            for i, paraBoundary in enumerate(paraBoundaries): \n",
    "                if set(range(match[0], match[1])) & set(range(paraBoundary[0], paraBoundary[1])): # find the set intersection of the ranges of pairs\n",
    "                    paraIndices.append(i)\n",
    "                else: \n",
    "                    paraIndices.append(None)\n",
    "    return paraIndices\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(range(8, 10)) & set(range(1, 9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches['paraIndicesIn20'] = articlesWithMatches['Locations in A'].apply(paraIndicesIn20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = list(articlesWithMatches['paraIndicesIn20'].apply(Counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grandTally = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in counters: \n",
    "    grandTally += counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del grandTally[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(grandTally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dict(grandTally)).sort_index().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mm[xxParaLocations[5]:xxParaLocations[7]]) # What are paragraphs #5 and #6? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLH, ELH and GE-GHL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## George Eliot - George Henry Lewes Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### George Eliot - George Henry Lewes Studies articles where journal title is \"George Eliot - George Henry Lewes Studies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geJournals = df.loc[df['journal'] == 'George Eliot - George Henry Lewes Studies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geJournals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geJournals.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of George ELiot - George Henry Lewes Studies articles where journal title is 'George ELiot - George Henry Lewes Studies':\")\n",
    "len(geJournals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### George Eliot - George Henry Lewes Studies articles where journal code is \"georelioghlstud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of George Eliot - George Henry Lewes Studies articles where journal code is 'georelioghlstud':\")\n",
    "geJournalCodes = df.loc[df['jcode'].str[0] == 'georelioghlstud']\n",
    "len(geJournalCodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLH articles where journal title is \"New Literary History\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlhJournals = df.loc[df['journal'] == 'New Literary History']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlhJournals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of NLH articles where journal title is 'New Literary History':\")\n",
    "len(nlhJournals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLH articles where journal code is \"newlitehist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('NLH articles where journal code is \"newlitehist\":')\n",
    "nlhJournalCodes = df.loc[df['jcode'].str[0] == 'newlitehist']\n",
    "len(nlhJournalCodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELH articles where journal title is \"ELH\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elhJournals = df.loc[df['journal'] == 'ELH']\n",
    "elhJournals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elhJournals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELH articles where journal code is \"elh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elhJournalCodes = df.loc[df['jcode'].str[0] == 'elh']\n",
    "len(elhJournalCodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonmatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df # Print the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.title.apply(isGarbage)] # How many garbage items? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to find out what articles contain no Middlemarch citations\n",
    "articlesWithoutMatches = df[df['Locations in A'].apply(lambda x: len(x) == 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "articlesWithoutMatches['title'].value_counts()[:n].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most frequent name of articles with no citations?\n",
    "articlesWithoutMatches['title'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating samples of dataset for evaluating the precision and recall of text matcher\n",
    "First, we're going to generate a smaller sample dataset, which we'll then perform bootstrapping on.\n",
    "\n",
    "First, let's stratify our dataset by year, and then take a random sample in that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articlesWithMatches1960_2015 = articlesWithMatches[articlesWithMatches['Decade'] >= 1960]\n",
    "len(articlesWithMatches1960_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articlesWithMatches1960_2015['year'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData = articlesWithMatches1960_2015.sample(n=56, random_state=56)\n",
    "sampleData['journal'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleData.to_csv('../data/sample_dataset.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to loop over each row, extracting locations in A and metadata, then output that to a new text file\n",
    "def extractSampleDataMatches(sampleData):\n",
    "    for i, row in sampleData.iterrows():\n",
    "        title = row['title']\n",
    "        year = row['year']\n",
    "        # Print a break between each article\n",
    "        with open('../data/sample-data-matches.txt', \"a\") as f:\n",
    "            print(\"---------------------------------------\\n\", file=f)\n",
    "            print(title, file=f)\n",
    "            print(year, file=f)\n",
    "        # For each pair of locations in the \"Locations in A\" column, iterate over, printing the location indexes\n",
    "        # Followed by the\n",
    "            for pair in row['Locations in A']:\n",
    "                print(f\"Location in A: {pair}\", file=f)\n",
    "                print(mm[pair[0]:pair[1]]+\"\\n\", file=f)\n",
    "    \n",
    "extractSampleDataMatches(sampleData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics\n",
    "\n",
    "Terminology\n",
    "TP (True Positives):\n",
    "TN (True Negatives): \n",
    "FP (False Posiives): \n",
    "FN (False Negatives): \n",
    "\n",
    "**Classification accuracy:** percentage of correctly identified quotes and non-quotes, or overall, how often is the matcher correct? classification_accuracy = (TP + TN) / float(TP + TN + FP + FN)))\n",
    "\n",
    " **Recall (or \"sensitivity\")**: When the actual match is correc, how often is the prediction correct? recall = TP / float(FN + TP)\n",
    "\n",
    "\n",
    "**Precision:** When a match is detected, how often is that match correct? precision = TP / float(TP + FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
